# Scout Mindset

Author: Julia Galef

`Below are some of the notes I collected when reading/understanding/learning this book, some copy-pasted some paraphrased.`

Scout mindset: the motivation to see things as they are, not as you wish they were.

Scout mindset is what allows you to recognize when you're wrong, it strips your thinking off your biases, it's what you use if you want to test your assumptions, if you want to be objective about something, when you would like to change your mind about something, It lets you ask important questions like “Was I at fault in that argument?” or “Is this risk worth it?” or “How would I react if someone from the other political party did the same thing?”

> “The first principle is that you must not fool yourself—and you are the easiest person to fool." -Richard Feynman

More often than not we try to justify things that we know or we might not know are wrong, human brain hardwired for self-deception: We rationalize away our flaws and mistakes. We indulge in wishful thinking. We cherry-pick evidence that confirms our prejudices and supports our political tribe.

Scout mindset is what you need to stop fooling yourself, to confront the truth, it's not something you've not been doing, in fact you might have found yourself questioning your choices, your mistakes but we don't do it as much as we should be doing.

Knowing _how to reason_ isn't a cure-all, just like knowing the importance of exercising isn't going to make you healhty. Unless you acknowledge your biases and fallacies in your thinking and put in conscious effort to fight them off.

> Our judgment isn’t limited by knowledge nearly as much as it’s limited by attitude. - Julia Galef

- The Truth isn't in conflict with your other goals.
- Learn tools that make it easier to see clearly.
  - eg: outsider test, the selective skeptic test, the conformity test, introspection techniques
- Appreciating the emotional rewards of scout mindset.

Scout mindset is rewarding in the sense that it allows us to be open minded no matter the situation we're in, there's equanimity that results from understanding risk and coming to terms with the odds you’re facing. And there’s a refreshing lightness in the feeling of being free to explore ideas and follow the evidence wherever it leads, unconstrained by what you’re “supposed to” think.

It's a different way of being, one that’s rooted in an appetite for truth, and one that’s both useful and fulfilling

To be willing to consider other interpretations—to even believe that there could be other reasonable interpretations besides your own—requires scout mindset.

---

The opposite of [[directionally-motivated-reasoning]] is [[accuracy-motivated-reasoning]]

---

## Quick Overview of Soldier vs Scout Mindset

| Soldier Mindset                                                                                                    | Scout Mindset                                                                                               |
| ------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| Reasoning is like defensive combat.                                                                                | Reasoning is like mapmaking.                                                                                |
| Decide what to believe by asking either “Can I believe this?” or “Must I believe this?” depending on your motives. | Decide what to believe by asking, “Is this true?”                                                           |
| Finding out you’re wrong means suffering a defeat.                                                                 | Finding out you’re wrong means revising your map.                                                           |
| Seek out evidence to fortify and defend your beliefs.                                                              | Seek out evidence that will make your map more accurate.                                                    |
| Related concepts: Directionally motivated reasoning, rationalizing, denial, self-deception, wishful thinking       | Related concepts: Accuracy motivated reasoning, truth-seeking, discovery, objectivity, intellectual honesty |

---

While we're not all perfect scouts, we should always try, in all contexts of our lives to stay in the scout mindset.

To let go of the [[soldier-mindset]] we need to ask why it was there in the first place.

## We make unconscious trade-offs

We trade off between judgment and belonging. If you live in a tight-knit community, it might be easier to fit in if you use soldier mindset to fight off any doubts you have about your community’s core beliefs and values. On the other hand, if you do allow yourself to entertain those doubts, you might realize you’re better off rejecting your community’s views on morality, religion, or gender roles, and deciding to live a less traditional life.

- We trade off between judgment and persuasion.
- We trade off between judgment and morale.
  - When you come up with a plan, focusing only on its positives (“This is such a great idea!”) can help you work up enthusiasm and motivation to carry it out. On the other hand, if you scrutinize your plan for flaws (“What are the downsides? How might this fail?”), you’re more likely to notice if there’s a better plan you should switch to instead.

We make these trade-offs, and many more, all the time, usually without even realizing we’re doing so. After all, the whole point of self-deception is that it’s occurring beneath our conscious awareness. If you were to find yourself thinking, explicitly, “Should I admit to myself that I screwed up?” the issue would already be moot. So it’s left up to our unconscious minds to choose, on a case-by-case basis, which goals to prioritize. Sometimes we choose soldier mindset, furthering our emotional or social goals at the expense of accuracy. Sometimes we choose scout mindset, seeking out the truth even if it turns out not to be what we were hoping for.

And sometimes our unconscious minds attempt to have it both ways, our desire to protect our self-esteem and happiness can be in competition with our desire to learn about problems we're facing(so that We can fix them)

## Are we rationally irrational?

The hypothesis that the human mind evolved the ability to make these trade-offs well is called the rational irrationality hypothesis, if the name sounds like a paradox, that’s because it’s using two different senses of the word rational:[[espistemic-rationality]] means holding beliefs that are well justified, while [[instrumental-rationality]] means acting effectively to achieve your goals.

Being rationally irrational, therefore, would mean that we’re good at unconsciously choosing just enough epistemic irrationality to achieve our social and emotional goals, without impairing our judgment too much. A rationally irrational person would deny problems only when the comfort of denial is sufficiently high and their chance of fixing the problem is sufficiently low.

So, are we rationally irrational?

we’re far from rationally irrational. There are several major biases in our decision-making, several ways in which we systematically misjudge the costs and benefits of truth.
Our biases cause us to overvalue soldier mindset, choosing it more often than we should, and undervalue scout mindset, choosing it less often than we should.

## We overvalue the immediate rewards of solider mindset

One of the most frustrating aspects of being human is our knack for undermining our own goals.

The source of this self-sabotage is present bias, a feature of our intuitive decision-making in which we care too much about short-term consequences and too little about long-term consequences. In other words, we’re impatient, and we get more impatient as the potential rewards grow closer.

When you contemplate a gym membership, the trade-off seems well worth it, in theory. Spend a few hours per week exercising, in exchange for looking and feeling a lot better? Sign me up! But on any given morning, when you’re faced with the choice between “Turn off my alarm and sink blissfully back into sleep” or “Head to the gym and make an imperceptible amount of progress toward my fitness goals,” it’s a much tougher call. The rewards of choosing to sleep in are immediate; the rewards of choosing to exercise are diffuse and delayed. What difference will one exercise session make to your long-term fitness goals, anyway?

It’s widely known that present bias shapes our choices about how to act. What’s much less appreciated is that it also shapes our choices about how to think. Just like sleeping in, breaking your diet, or procrastinating on your work, we reap the rewards of thinking in soldier mindset right away, while the costs don’t come due until later. If you’re worried about a mistake you made and you convince yourself that “It wasn’t my fault,” you’re rewarded with a hit of instant emotional relief. The cost is that you miss out on learning from your mistake, which means you’re less able to prevent it from happening again. But that won’t affect you until some unknown point in the future.

Being overly optimistic about your chance of success gives you a burst of motivation right away. But those motivational benefits dwindle over time, or even backfire, when success takes longer than you predicted. As Francis Bacon said, “Hope is a good breakfast, but a bad supper.”

## We underestimate the value of building scout habits

When you wake up in the morning and head to the gym, the benefit of that choice isn’t just in the calories you burn or the muscle tone you develop that day. The benefit also lies in the fact that you’re reinforcing valuable skills and habits. That includes the habit of going to the gym, obviously, but also the broader skill of doing hard things, and the broader habit of following through on your promises to yourself.

Any single day, on its own, doesn’t make much difference to your overall habits and skills. “I can just go tomorrow,” you think as you turn off the alarm. Which is true—but of course, you’ll think the same thing tomorrow.

Even when you’re reasoning about something like foreign politics that doesn’t impact your life directly, the way you think still impacts you indirectly, because you’re reinforcing general habits of thought. Every time you say, “Oh, that’s a good point, I hadn’t thought of that,” it gets a little bit easier for you to acknowledge good points in general. Every time you opt to check a fact before citing it, you become a little bit more likely to remember to check your facts in general. Every time you’re willing to say, “I was wrong,” it gets a little bit easier to be wrong in general.

It’s hard for the “Incrementally improve my thinking habits” benefit to compete with the vivid and immediate rewards of soldier mindset.

## we underestimate the ripple effects of self-deception

When you tell a lie, it’s hard to predict exactly what you’ve committed your future self to.

Just like the lies we tell others, the lies we tell ourselves have ripple effects. Suppose you tend to rationalize away your own mistakes, and consequently you see yourself as more perfect than you really are. This has a ripple effect on your views of other people: Now, when your friends and family screw up, you might not be very sympathetic. After all, you never make such mistakes. Why can’t they just be better? It’s not that hard.

Or suppose that for the sake of your self-esteem, you view yourself through rose-colored glasses, judging yourself to be more charming, interesting, and impressive than you actually appear to other people. Here’s one possible ripple effect: How do you explain the fact that women don’t seem to be interested in dating you, given what a great catch you are? Well, maybe they’re all shallow.

It’s hard to know exactly how the ripple effect of a particular act of self-deception will hurt you in the future, or if it will at all. Perhaps in many instances the harm is negligible. But the fact that the harm is delayed and unpredictable should ring an alarm bell. This is exactly the kind of cost we tend to neglect when we’re intuitively weighing costs and benefits. Ripple effects are yet more reason to suspect that we underestimate the cost of deceiving ourselves—and are therefore choosing soldier mindset too often and scout mindset not often enough.

## We overestimate social costs

Another way in which our intuition about costs and benefits is skewed—we overestimate the importance of how we come across to other people. Social costs like looking weird or making a fool out of ourselves feel a lot more significant than they actually are. In reality, other people aren’t thinking about you nearly as much as you intuitively think they are, and their opinions of you don’t have nearly as much impact on your life as it feels like they do.

As a result, we end up making tragic trade-offs, sacrificing a lot of potential happiness to avoid relatively small social costs.

When we allow ourselves to reflect on a social cost we’ve been avoiding (or when someone else prompts us to reflect on it, we often realize, “Hey, this isn’t such a big deal after all. I can decide to take on a little more responsibility at work, and it’ll be fine. No one’s going to hate me for that.” But when we leave the decision up to our instincts, even a hint of potential social risk prompts a reflexive “Avoid at all costs!” reaction.

---

We’re overly tempted by immediate payoffs, even when they come at a steep cost later on. We underestimate the cumulative harm of false beliefs, and the cumulative benefit of practicing scout habits. We overestimate how much other people judge us, and how much impact their judgments have on our lives. As a result of all these tendencies, we end up being far too willing to sacrifice our ability to see clearly in exchange for short-term emotional and social rewards. That doesn’t mean scout mindset is always the better choice—but it does mean we have a bias in favor of the soldier, even when the scout is a better choice.

## An accurate map is more useful now

We have far more options now.

Whether your choices make your life better or worse depends on your judgment, and your judgment depends on your mindset.

Living in the modern world also means we have many more opportunities to fix things we don’t like about our lives. If you’re bad at something, you can take classes, read a For Dummies book, watch a YouTube tutorial, get a tutor, or hire someone to do it for you. If you’re chafing under the constrictive social mores of your town, you can find kindred spirits online or move to a big city. If your family is abusive, you can cut ties with them.

Not all of these solutions are equally effective for everyone, and not all of them are worth the effort or cost. Deciding which solutions are worth trying is a matter of judgment. Deciding which problems in your life are worth trying to solve at all, versus simply learning to live with, is a matter of judgment, too.

This abundance of opportunity makes scout mindset far more useful than it would have been for our ancestors. After all, what’s the point of admitting your problems exist if you can’t fix them? What’s the point of noticing your disagreements with your community if you can’t leave? Having an accurate map doesn’t help you very much when you’re allowed to travel only one path.

So if our instincts undervalue truth, that’s not surprising—our instincts evolved in a different world, one better suited to the soldier. Increasingly, our world is becoming one that rewards the ability to see clearly, especially in the long run; a world in which your happiness isn’t nearly as dependent on your ability to accommodate yourself to whatever life, skills, and social groups you happened to be born into.

More and more, it’s a scout’s world now.

## Feeling objective doesn't make you a scout

We think of ourselves as objective because we feel objective. We scrutinize our own logic and it seems sound. We don’t detect any signs of bias in ourselves. We feel unemotional, dispassionate.
But the fact that you feel calm doesn’t mean you’re being fair.

being able to explain a position “rationally,” - by which people usually mean that they can make a compelling argument in favor of their position—doesn’t mean the position is fair. Of course your argument seems compelling to you; everyone’s argument seems compelling to them. That’s how motivated reasoning works.

In fact, viewing yourself as rational can backfire. The more objective you think you are, the more you trust your own intuitions and opinions as accurate representations of reality, and the less inclined you are to question them.

When you start from the premise that you’re an objective thinker, you lend your conclusions an air of unimpeachability they usually don’t deserve.

## Being smart and knowledgeable doesn't make you a scout

when you ask people for their opinions on other ideologically charged scientific issues: Should the government fund stem cell research? How did the universe begin? Did humans evolve from lower animal species? On all these questions, the people with the highest levels of scientific intelligence were also the most politically polarized in their opinions.

Truth doesn't always lie in the center. If anyone claims so, that would be false balance.

On any particular issue, the truth may lie close to the far left or the far right or anywhere else. The point is simply that as people become better informed, they should start to converge on the truth, wherever it happens to be. Instead, we see the opposite pattern—as people get better informed, they diverge.

This is a crucially important result, because being smart and being knowledgeable on a particular topic are two more things that give us a false sense of security in our own reasoning. A high IQ and an advanced degree might give you an advantage in ideologically neutral domains like solving math problems or figuring out where to invest your money. But they won’t protect you from bias on ideologically charged questions.

Intelligence and knowledge are just tools. You can use those tools to help you see the world clearly, if that’s what you’re motivated to do. Or you can use them to defend a particular viewpoint, if you’re motivated to do that instead. But there’s nothing inherent to the tools that makes you a scout.

## Actually practicing scout mindset makes you a scout

It’s easy to think, “Of course I change my mind in response to evidence,” or “Of course I apply my principles consistently,” or “Of course I’m fair-minded,” whether or not those things are true. The test of scout mindset isn’t whether you see yourself as the kind of person who does these things. It’s whether you can point to concrete cases in which you did, in fact, do these things.

Feeling reasonable, being smart and knowledgeable, being aware of motivated reasoning—all these things seem like they should be indicators of scout mindset, yet they have surprisingly little to do with it. The only real sign of a scout is whether you act like one.

Five signs of scout mindset, behavioral cues that someone cares about truth and will seek it out even when they’re not forced to, and even when the truth isn’t favorable to them.

- ### 1. Do you tell other people when you realize they were right?

Technically, scout mindset only requires you to be able to acknowledge to yourself that you were wrong, not to other people. Still, a willingness to say “I was wrong” to someone else is a strong sign of a person who prizes the truth over their own ego. Can you think of cases in which you’ve done the same?

- ### 2. How do you react to personal criticism?

It’s a lot easier to say you welcome criticism than it is to actually welcome it. But in so many domains, getting honest feedback is essential to improvement.

To gauge your comfort with criticism, it’s not enough just to ask yourself, “Am I open to criticism?” Instead, examine your track record. Are there examples of criticism you’ve acted upon? Have you rewarded a critic (for example, by promoting him)? Do you go out of your way to make it easier for other people to criticize you?

- ### 3. Do you ever prove yourself wrong?

“We all identified with something because it sounded like our reality,”

Can you think of any examples in which you voluntarily proved yourself wrong? Perhaps you were about to voice an opinion online, but decided to search for counterarguments first, and ended up finding them compelling. Or perhaps at work you were advocating for a new strategy, but changed your mind after you ran the numbers more carefully and realized it wouldn’t be feasible.

- ### 4. Do you take precautions to avoid fooling yourself?

Do you try to avoid biasing the information you get? For example, when you ask your friend to weigh in on a fight you had with your partner, do you describe the disagreement without revealing which side you were on, so as to avoid influencing your friend’s answer? When you launch a new project at work, do you decide ahead of time what will count as a success and what will count as a failure, so you’re not tempted to move the goalposts later?

- ### 5. Do you have any good critics?

It’s tempting to view your critics as mean-spirited, ill-informed, or unreasonable. And it’s likely that some of them are. But it’s unlikely that all of them are. Can you name people who are critical of your beliefs, profession, or life choices who you consider thoughtful, even if you believe they’re wrong? Or can you at least name reasons why someone might disagree with you that you would consider reasonable (even if you don’t happen to know of specific people who hold those views)?

---

Being able to name reasonable critics, being willing to say “The other side has a point this time,” being willing to acknowledge when you were wrong—it’s things like these that distinguish people who actually care about truth from people who only think they do.

But the biggest sign of scout mindset may be this: Can you point to occasions in which you were in soldier mindset? If that sounds backward, remember that motivated reasoning is our natural state. It’s universal, hardwired into our brains. So if you never notice yourself doing it, what’s more likely—that you happen to be wired differently from the rest of humanity or that you’re simply not as self-aware as you could be?

Learning to spot your own biases, in the moment, is no easy feat. But it’s not impossible, if you have the right tools. That’s what the next two chapters are about.

---

## Noticing biases

If you get sued and you win the case, should the person who sued you pay your legal costs? If you’re like most people (85 percent, in one study1), your answer is yes. After all, if you’re falsely accused of something, why should you be out thousands of dollars in lawyers’ fees? That wouldn’t be fair.

However, when the question in that study was slightly reworded—“If you sue someone and you lose the case, should you pay his costs?”—only 44 percent of people said yes. Imagining yourself in the role of the person who sued and lost brings to mind alternate arguments. For example, you might have lost simply because the other side is wealthy and can afford better lawyers. It’s not fair to discourage victims from suing just because they can’t afford to lose, right?

Both the arguments for and against the “loser pays” policy have at least some merit. But which one comes to mind will depend on whether you’re the plaintiff or the defendant—and it will likely never occur to you that you could have thought of an opposing argument if you had been on the other side of the case.

## A thought experiment is a peak into the counterfactual world

You can’t detect motivated reasoning in yourself just by scrutinizing your reasoning and concluding that it makes sense. You have to compare your reasoning to the way you would have reasoned in a counterfactual world, a world in which your motivations were different—would you judge that politician’s actions differently if he was in the opposite party? Would you evaluate that advice differently if your friend had offered it instead of your spouse? Would you consider that study’s methodology sound if its conclusions supported your side?

You can’t literally visit the counterfactual world. But you can do the next best thing—peek into it virtually, with a thought experiment.

lets explore five different types of thought experiments: the double standard test, the outsider test, the conformity test, the selective skeptic test, and the status quo bias test.

Thought experiments only work if you actually do them. So don’t simply formulate a verbal question for yourself. Conjure up the counterfactual world, place yourself in it, and observe your reaction.

---

## Common thought experiments

| Name                       | Definition                                                                                                      |
| -------------------------- | --------------------------------------------------------------------------------------------------------------- |
| The Outsider Test          | How would you evaluate this situation if it wasn’t your situation?                                              |
| The Conformity Test        | If other people no longer held this view, would you still hold it?                                              |
| The Selective Skeptic Test | If this evidence supported the other side, how credible would you judge it to be?                               |
| The Status Quo Bias Test   | If your current situation was not the status quo, would you actively choose it?                                 |
| The Double Standard Test   | Are you judging one person (or group) by a different standard than you would use for another person (or group)? |

---

What thought experiments do is simply reveal that your reasoning changes as your motivations change. That the principles you’re inclined to invoke or the objections that spring to your mind depend on your motives: the motive to defend your image or your in-group’s status; the motive to advocate for a self-serving policy; fear of change or rejection.

Catching your brain in the act of motivated reasoning—noticing when an experiment’s previously invisible flaws jump out at you, or noticing that your preferences change as you switch around supposedly irrelevant details of a scenario—breaks down the illusion that your initial judgment is the objective truth. It convinces you, viscerally, that your reasoning is contingent; that your initial judgments are a starting point for exploration, not an end point.

# How Sure Are You?

## We like feeling certain

Overconfidence, when confidence that one is right outstrips the actual accuracy. Doctors

The certainty we express is partly just for simplicity’s sake. Conversation would be unwieldy if we had to stop and assign a probability to every statement we made. But even when someone does prompt us to stop and reflect on our level of confidence, we often claim to be completely certain.

Even professionals are frequently certain and wrong in their area of expertise. For example, many studies have found that doctors routinely overestimate their ability to diagnose patients.

If we tend to be overly certain about our knowledge, that’s even more the case when it comes to our opinions.

Not all overconfidence is due to motivated reasoning. Sometimes we simply don’t realize how complicated a topic is, so we overestimate how easy it is to get the right answer. But a large portion of overconfidence stems from a desire to feel certain. Certainty is simple. Certainty is comfortable. Certainty makes us feel smart and competent.

Your strength as a scout is in your ability to resist that temptation, to push past your initial judgment, and to think in shades of gray instead of black and white. To distinguish the feeling of “95% sure” from “75% sure” from “55% sure.”

## Quantifying your uncertainty

A scout treats their degree of certainty as a prediction of their likelihood of being right.

Imagine sorting all of your beliefs into buckets based on how sure you are that you’re right about each one. This would include quotidian predictions (“I will enjoy this restaurant”), beliefs about your life (“My partner is faithful to me”), beliefs about how the world works (“Smoking causes cancer”), core premises (“Magic isn’t real”), and so on. Putting a belief into the “70% sure” bucket is like saying, “This is the kind of thing I expect to get right roughly 70 percent of the time.”

What you’re implicitly aiming for when you tag your beliefs with various confidence levels is perfect calibration. That means your “50% sure” claims are in fact correct 50 percent of the time, your “60% sure” claims are correct 60 percent of the time, your “70% sure” claims are correct 70 percent of the time, and so on.

Perfect calibration is an abstract ideal, not something that’s possible to achieve in reality. Still, it’s a useful benchmark against which to compare yourself.

## A bet can reveal how sure you really are

Imagine you’re talking with a friend who’s struggling to get her new catering business off the ground. You reassure her, “You’re amazing at this stuff! The only reason business is slow is because you’re just starting out. Everyone has trouble getting clients at first!”

She replies, “Thanks! I’m so glad you feel that way. Could you recommend me to your coworkers?”

Suddenly you feel hesitant. You recall her telling you about backing out of a job at the last minute . . . and you realize that you’ve never actually tasted her cooking . . . and you can’t help but ask yourself, “How sure am I, really, that she’ll do a decent job?”

When you were reassuring your friend a moment earlier, you weren’t lying. You just weren’t thinking about what you actually believed, because it didn’t seem to matter. But once there are actual stakes, and your reputation could take a hit if you guess wrong about your friend’s catering skill, your brain switches over from the goal “be supportive” to the goal “actually try to get the right answer.”

Imagine that a company sells toothpaste. The press secretary might assert confidently, “Our toothpaste whitens teeth better than any other brand on the market.” But suppose the board is approached by a dental school professor who says, “I’d like to do a study. I’ll assign groups of people to use one of the leading brands of toothpaste, without telling them which brand it is, and then I’ll measure how much whiter their teeth are. I’ll publish whatever results I get.”

If the board was truly confident that their toothpaste worked best, they would say, “Great—a chance to prove to the public that we’re the best!” But despite the press secretary’s assurances, the board might decide that they’re not confident enough they would win such a contest, and it’s not worth risking the embarrassment of losing.

The press secretary isn’t thinking about what’s true. He’s thinking about what he can get away with saying, what will present the company in the best light while still being at least sort of plausible. But the board is incentivized to form their best guess about the truth, because the company will thrive if they guess right and suffer if they’re wrong. The press secretary makes claims; the board makes bets.

The word bet might conjure up horse races and blackjack tables, but its meaning is far more general. A bet is any decision in which you stand to gain or lose something of value, based on the outcome. That could include money, health, time—or reputation, as in the case of your catering friend who wants your endorsement. So when you’re thinking about how sure you are, your answer will be more honest if you switch from thinking in terms of “What can I get away with claiming to myself?” to “How would I bet, if there was something at stake?”

A tip when you’re imagining betting on your beliefs: You may need to get more concrete about what you believe by coming up with a hypothetical test that could be performed to prove you right or wrong. For example, if you believe “Our computer servers are highly secure,” a hypothetical test might be something like this: Suppose you were to hire a hacker to try to break in to your systems. If they succeed, you lose one month’s salary. How confident do you feel that you would win that bet?

## The equivalent bet test

The examples of bets in the previous section are meant to generate a qualitative sense of your confidence in a belief. Do you feel happy to take the bet, without hesitating? Do you feel a little bit of doubt? Do you feel truly torn? Your hesitation, or lack thereof, is a proxy for your degree of confidence that your belief is true.

Contemplating a bet can also be used to pin down how sure you are quantitatively, helping you put a number on your degree of confidence.

I can bet on self-driving cars, and get $10,000 if they’re on the market in a year. Alternately, I can take the “ball bet”: I’m given a box containing four balls, one of which is gray. I reach in and pull out one ball, without looking—if it’s gray, I win $10,000.

Which gamble would I rather take? I hesitate for a moment, but I feel happier with the ball bet. Since the probability of winning the ball bet is 1 in 4 (or 25 percent), the fact that I feel more confident in the ball bet implies that I’m less than 25 percent confident in self-driving cars making it to the market in a year.

Let’s try decreasing the odds of winning the ball bet. Suppose the box contains sixteen balls, only one of which is gray. Now which do I prefer: betting on drawing the gray ball or betting on self-driving cars in a year?

This time, I notice that I prefer my chances on self-driving cars. After all, sometimes technological progress surprises us. Maybe one of the companies working on self-driving technology is actually farther along than they’ve been letting on. It seems unlikely, but I’d still rather gamble on that than on being lucky enough to draw the gray ball. And since the probability of drawing a gray ball is 1 in 16 (or about 6 percent), the fact that I would prefer to bet on self-driving cars implies that I am more than 6 percent confident that self-driving cars will make it to the market in a year.

Okay, let’s adjust the chances of winning the ball bet back upward a little bit, to one in nine. Now which do I prefer?

Hmm. I’m really torn. Neither seems like a clearly better bet. The bets feel equivalent to me—and since we know that the probability of winning the ball bet is 1 in 9 (or about 11 percent), that implies that I have roughly 11 percent confidence in self-driving cars coming out within a year. I still don’t think the “Self-driving cars will be on the market in a year” forecast is likely to come true, but I’ve gone from a glib “That’s crazy” to a more honest best guess.

being able to tell the difference between the feeling of making a claim and the feeling of actually trying to guess what’s true. Making a claim feels like your press secretary is speaking. It feels pat; neat and tidy. Sometimes hurried, as if you’re trying to put something past yourself. The mental motion is declaring, proclaiming, insisting, or perhaps scoffing.

---

# How Sure Are You?

In a scene in the 2016 movie Star Trek Beyond, a spaceship careens across the sky.1 It’s being piloted by Captain Kirk, who is hot on the tail of three enemy ships headed straight for the center of a city, where they intend to detonate a superweapon. Kirk’s right-hand man, Commander Spock, yells to him: “Captain, intercepting all three ships is an impossibility!”

An impossibility. The words sound so authoritative, so definitive. And yet less than sixty seconds later, Kirk has figured out how to maneuver in front of the enemy ships, stopping them with his own ship’s hull before they can reach their destination.

If you’ve watched much Star Trek before, this won’t surprise you. Spock doesn’t have a great track record when it comes to making accurate predictions. “There’s only a very slight chance this will work,” Spock warns Kirk in one episode of the original TV show, right before their plan works.2 The odds of survival are “less than seven thousand to one,” Spock tells Kirk in another episode, shortly before they escape unharmed.3 The chance of finding survivors is “absolutely none,” Spock declares in yet another episode, right before they discover a large colony of survivors.4

## We like feeling certain

Spock is overconfident, meaning that his confidence that he’s right outstrips his actual accuracy. In that respect, Spock isn’t all that different from most of us (except that he makes a much bigger deal about how his predictions are objective and “logical,” which is why I’ve chosen to make an example of him). We very often speak as if there’s no chance we could be mistaken—“There’s no way he can make that shot from that distance!” or “I’ll definitely have that done by Friday”—and yet we turn out to be wrong nonetheless.

To be fair, the certainty we express is partly just for simplicity’s sake. Conversation would be unwieldy if we had to stop and assign a probability to every statement we made. But even when someone does prompt us to stop and reflect on our level of confidence, we often claim to be completely certain. You’ll notice this if you search online for phrases like, “How certain are you that” or “How confident are you that.” Here are a few examples I pulled from discussions on Quora, Yahoo! Answers, Reddit, and other forums:

As a percentage, how certain are you that intelligent life exists outside of Earth? “I am 100% certain there’s other intelligent life.”5

How confident are you that you are going to hit your 2017 sales goals? “I’m 100% confident.”6

Atheists, how confident are you that you won’t convert to a religion like Christianity on your deathbed? “100% confident.”7

Even professionals are frequently certain and wrong in their area of expertise. For example, many studies have found that doctors routinely overestimate their ability to diagnose patients. One study examined the autopsy results for patients who had been given diagnoses with “complete certainty,” and found that in 40 percent of those cases, the diagnosis was incorrect.8

If we tend to be overly certain about our knowledge, that’s even more the case when it comes to our opinions. We say things like, “There is no question that America needs a living wage,” or “It’s obvious that the internet has wrecked our attention spans,” or “Of course that bill would be a disaster.”

Not all overconfidence is due to motivated reasoning. Sometimes we simply don’t realize how complicated a topic is, so we overestimate how easy it is to get the right answer. But a large portion of overconfidence stems from a desire to feel certain. Certainty is simple. Certainty is comfortable. Certainty makes us feel smart and competent.

Your strength as a scout is in your ability to resist that temptation, to push past your initial judgment, and to think in shades of gray instead of black and white. To distinguish the feeling of “95% sure” from “75% sure” from “55% sure.” That’s what we’ll learn to do in this chapter.

But first, let’s back up—what does it even mean to put a number on your degree of belief?

## Quantifying your uncertainty

Typically, when people think about how sure they are, they ask themselves something like: “Do I actively feel any doubt?” If the answer is no, as it often is, they declare themselves to be “100% certain.”

That’s an understandable way to think about certainty, but it’s not the way a scout thinks about it. A scout treats their degree of certainty as a prediction of their likelihood of being right. Imagine sorting all of your beliefs into buckets based on how sure you are that you’re right about each one. This would include quotidian predictions (“I will enjoy this restaurant”), beliefs about your life (“My partner is faithful to me”), beliefs about how the world works (“Smoking causes cancer”), core premises (“Magic isn’t real”), and so on. Putting a belief into the “70% sure” bucket is like saying, “This is the kind of thing I expect to get right roughly 70 percent of the time.”

What you’re implicitly aiming for when you tag your beliefs with various confidence levels is perfect calibration. That means your “50% sure” claims are in fact correct 50 percent of the time, your “60% sure” claims are correct 60 percent of the time, your “70% sure” claims are correct 70 percent of the time, and so on.

## Perfect calibration

Perfect calibration is an abstract ideal, not something that’s possible to achieve in reality. Still, it’s a useful benchmark against which to compare yourself. To get a hang of the concept, let’s continue picking on Spock and see how his calibration measures up against perfection.

## A bet can reveal how sure you really are

Imagine you’re talking with a friend who’s struggling to get her new catering business off the ground. You reassure her, “You’re amazing at this stuff! The only reason business is slow is because you’re just starting out. Everyone has trouble getting clients at first!”

She replies, “Thanks! I’m so glad you feel that way. Could you recommend me to your coworkers?”

Suddenly you feel hesitant. You recall her telling you about backing out of a job at the last minute . . . and you realize that you’ve never actually tasted her cooking . . . and you can’t help but ask yourself, “How sure am I, really, that she’ll do a decent job?”

When you were reassuring your friend a moment earlier, you weren’t lying. You just weren’t thinking about what you actually believed, because it didn’t seem to matter. But once there are actual stakes, and your reputation could take a hit if you guess wrong about your friend’s catering skill, your brain switches over from the goal “be supportive” to the goal “actually try to get the right answer.”

Evolutionary psychologist Robert Kurzban has an analogy for these two modes.11 In a company, there’s a board of directors whose role is to make the crucial decisions for the company—how to spend its budget, which risks to take, when to change strategies, and so on. Then there’s a press secretary whose role is to give statements about the company’s values, its mission, and the reasoning behind its decisions.

If a competitor starts gaining market share, the company’s press secretary might assure the public, “We’re not worried. Our brand has been America’s favorite for thirty years, and that’s not going to change.” However, if you were to sit in on a board meeting, you might find that behind the scenes, the board is taking the risk seriously and looking for ways to cut costs.

Imagine that the company sells toothpaste. The press secretary might assert confidently, “Our toothpaste whitens teeth better than any other brand on the market.” But suppose the board is approached by a dental school professor who says, “I’d like to do a study. I’ll assign groups of people to use one of the leading brands of toothpaste, without telling them which brand it is, and then I’ll measure how much whiter their teeth are. I’ll publish whatever results I get.”

If the board was truly confident that their toothpaste worked best, they would say, “Great—a chance to prove to the public that we’re the best!” But despite the press secretary’s assurances, the board might decide that they’re not confident enough they would win such a contest, and it’s not worth risking the embarrassment of losing.

The press secretary isn’t thinking about what’s true. He’s thinking about what he can get away with saying, what will present the company in the best light while still being at least sort of plausible. But the board is incentivized to form their best guess about the truth, because the company will thrive if they guess right and suffer if they’re wrong. The press secretary makes claims; the board makes bets.

The word bet might conjure up horse races and blackjack tables, but its meaning is far more general. A bet is any decision in which you stand to gain or lose something of value, based on the outcome. That could include money, health, time—or reputation, as in the case of your catering friend who wants your endorsement. So when you’re thinking about how sure you are, your answer will be more honest if you switch from thinking in terms of “What can I get away with claiming to myself?” to “How would I bet, if there was something at stake?”

Sometimes a project I’m working on seems hopeless. For example—just to pull a random, hypothetical situation out of thin air: “The book I’m writing is terrible and I should give up.” But how sure am I that I’m not just in a temporary funk? I’m 100% sure, my press secretary insists—but let’s ignore him, and pose a question to the board instead: “Suppose you would win $1,000 for correctly guessing whether you will still feel this way about your book a week from now. How would you bet?”

Now that there’s money on the line, I hesitate. I recall that I have felt pessimistic about my book, or some other project, many times in the past, and the dark cloud usually goes away in a day or two. It feels like a better bet to pick “Yes, I probably will feel better.” Going through that exercise doesn’t magically get rid of my bad mood, but it does take the edge off it. It’s useful to have proven to myself that I wouldn’t be willing to bet on this mood lasting, even though it feels like it will last forever.

A tip when you’re imagining betting on your beliefs: You may need to get more concrete about what you believe by coming up with a hypothetical test that could be performed to prove you right or wrong. For example, if you believe “Our computer servers are highly secure,” a hypothetical test might be something like this: Suppose you were to hire a hacker to try to break in to your systems. If they succeed, you lose one month’s salary. How confident do you feel that you would win that bet?

If you believe “I was being reasonable in that fight with my partner, and he was being unreasonable,” a hypothetical test might go something like this: Suppose another person, an objective third party, is given all of the relevant details about the fight, and is asked to judge which of you two is being more reasonable. If he judges in your favor, you win $1,000; if not, you lose $1,000. How confident do you feel that you would win that bet?

THE EQUIVALENT BET TEST
The examples of bets in the previous section are meant to generate a qualitative sense of your confidence in a belief. Do you feel happy to take the bet, without hesitating? Do you feel a little bit of doubt? Do you feel truly torn? Your hesitation, or lack thereof, is a proxy for your degree of confidence that your belief is true.

Contemplating a bet can also be used to pin down how sure you are quantitatively, helping you put a number on your degree of confidence. Sometimes I hear an ambitious technological forecast like, “Self-driving cars will be on the market within the year!” My first reaction is often to scoff, “Well, that’s crazy.” But how sure am I that the forecast is wrong?

To answer that question, I imagine facing a choice between two possible bets. I use a technique I adapted from decision-making expert Douglas Hubbard called an “equivalent bet test.”12 Here’s how it works in this case: I can bet on self-driving cars, and get $10,000 if they’re on the market in a year. Alternately, I can take the “ball bet”: I’m given a box containing four balls, one of which is gray. I reach in and pull out one ball, without looking—if it’s gray, I win $10,000.\*

Ball bet (1 in 4 chance of winning):

Bet on self-driving cars

Draw from a box with four balls, one of which is gray. If I draw the gray ball, I get $10,000.

If fully self-driving cars are available for purchase in a year, I get $10,000.

Which gamble would I rather take? I hesitate for a moment, but I feel happier with the ball bet. Since the probability of winning the ball bet is 1 in 4 (or 25 percent), the fact that I feel more confident in the ball bet implies that I’m less than 25 percent confident in self-driving cars making it to the market in a year.

Let’s try decreasing the odds of winning the ball bet. Suppose the box contains sixteen balls, only one of which is gray. Now which do I prefer: betting on drawing the gray ball or betting on self-driving cars in a year?

Ball bet (1 in 16 chance of winning):

Bet on self-driving cars

Draw from a box with sixteen balls, one of which is gray. If I draw the gray ball, I get $10,000.

If fully self-driving cars are available for purchase in a year, I get $10,000.

This time, I notice that I prefer my chances on self-driving cars. After all, sometimes technological progress surprises us. Maybe one of the companies working on self-driving technology is actually farther along than they’ve been letting on. It seems unlikely, but I’d still rather gamble on that than on being lucky enough to draw the gray ball. And since the probability of drawing a gray ball is 1 in 16 (or about 6 percent), the fact that I would prefer to bet on self-driving cars implies that I am more than 6 percent confident that self-driving cars will make it to the market in a year.

Okay, let’s adjust the chances of winning the ball bet back upward a little bit, to one in nine. Now which do I prefer?

Ball bet (1 in 9 chance of winning):

Bet on self-driving cars

Draw from a box with nine balls, one of which is gray. If I draw the gray ball, I get $10,000.

If fully self-driving cars are available for purchase in a year, I get $10,000.

Hmm. I’m really torn. Neither seems like a clearly better bet. The bets feel equivalent to me—and since we know that the probability of winning the ball bet is 1 in 9 (or about 11 percent), that implies that I have roughly 11 percent confidence in self-driving cars coming out within a year. I still don’t think the “Self-driving cars will be on the market in a year” forecast is likely to come true, but I’ve gone from a glib “That’s crazy” to a more honest best guess.

•   •   •
The core skill of the previous chapter on thought experiments was a kind of self-awareness, a sense that your judgments are contingent—that what seems true or reasonable or fair or desirable can change when you mentally vary some feature of the question that should have been irrelevant. The specific thought experiments we covered are all useful tools that I and other people use regularly. But the underlying shift in how you view your mind’s output is even more useful.

There’s a core skill in this chapter, too: being able to tell the difference between the feeling of making a claim and the feeling of actually trying to guess what’s true. Making a claim feels like your press secretary is speaking. It feels pat; neat and tidy. Sometimes hurried, as if you’re trying to put something past yourself. The mental motion is declaring, proclaiming, insisting, or perhaps scoffing.

Trying to guess what’s true feels like being the board of directors, deciding how to bet. There’s at least a second or two when you don’t know what answer you’re going to end up giving. It’s like you’re squinting at the evidence, trying to summarize what you see. The mental motions involved are estimating, predicting, weighing, and deliberating.

Quantifying your uncertainty, getting calibrated, and coming up with hypothetical bets are all valuable skills in their own right. But having the self-awareness to be able to tell whether you’re describing reality honestly, to the best of your abilities, is even more valuable still.

# Coping with reality

## Keeping despair at bay

One of the most fundamental human needs is to feel like things are basically okay; That’s why most people in an emergency resort to various forms of motivated reasoning, like denial, wishful thinking, and rationalizing.

The cruel irony is that an emergency is when you most need to be clear-eyed. The more you rely on motivated reasoning, the more you degrade your ability to make judgment calls.

Commitment to finding ways of keeping despair at bay without distorting his map of reality. (“You are doing the best you can. You can only do the best you can.”)

## Honest VS. Self-deceptive ways of coping.

“Was it a mistake to quit my job?” “Did I offend him?” Someone criticizes us. We face an unpleasant choice. We fail at something. In reaction, we reach for a thought that keeps negative emotions at bay—a coping strategy.

In the book Mistakes Were Made (But Not by Me), psychologists Carol Tavris and Elliot Aronson explore self-justification, a type of motivated reasoning in which you convince yourself after the fact that you made the right choice. The book is mostly about the many downsides of self-justification—how it commits us to stick with bad decisions rather than changing course, and dooms us to repeat our mistakes instead of learning from them. Still, Tavris and Aronson conclude, we need at least some amount of self-justification for the sake of our mental health: “Without it, we would prolong the awful pangs of embarrassment. We would torture ourselves with regret over the road not taken or over how badly we navigated the road we did take.”

`But is it really true that we need self-justification to prevent us from “torturing ourselves with regret”? Couldn’t we just . . . learn to not torture ourselves with regret instead?`

In Thinking, Fast and Slow, Nobel Prize–winning psychologist Daniel Kahneman points out an emotional benefit of motivated reasoning: resilience. It’s easier to bounce back from a failure if you can blame it on anyone but yourself. He uses the example of a door-to-door salesperson, a job that involves long strings of rejection: “When one has just had a door slammed in one’s face by an angry homemaker, the thought that ‘she was an awful woman’ is clearly superior to ‘I am an inept salesperson.’”

But are those really our only two options? We could instead tell ourselves, “Yes, I screwed up that sale. But everyone makes mistakes.” Or “Yes, I screwed up that sale. Still, I’m improving—I used to get doors slammed in my face every day, and now it only happens every week!”

Surely we can find a way to bounce back from our setbacks that doesn’t require us to blame them on other people—an honest coping strategy.

> Whenever I have found out that I have blundered, or that my work has been imperfect, and when I have been contemptuously criticized, and even when I have been overpraised, so that I have felt mortified, it has been my greatest comfort to say hundreds of times to myself that “I have worked as hard and as well as I could, and no man can do more than this.”

Scouts aren’t invulnerable to fear, anxiety, insecurity, despair, or any of the other emotions that give rise to motivated reasoning, and they rely on coping strategies just like anyone else. They just take more care to select coping strategies that don’t mess with the accuracy of their judgment.

When a negative emotion strikes, it’s as if we hurriedly reach into the bucket to grab something, anything, to make ourselves feel better. We don’t pay much attention to the kind of coping strategy we pull out, and whether it involves self-deception or not. As long as it makes us feel better, and it’s halfway plausible, it’ll do.

there is an abundance of different coping strategies, and you don’t need to be so quick to go with the first thing you happen to pull out of the bucket. You can almost always find something comforting that doesn’t require self-deception if you rummage around in there just a bit longer.

![Bucket of coping strategies](https://user-images.githubusercontent.com/31969517/120054182-1195a780-c04c-11eb-8093-b887d039c5c6.png)

## Make a plan

Tere are self-deceptive ways of coping with the thought of something unpleasant, such as coming up with rationalizations for why a task isn’t actually necessary, or flat-out denial. But there are also honest coping strategies, like coming up with a hypothetical plan.

It’s striking how much the urge to conclude “That’s not true” diminishes once you feel like you have a concrete plan for what you would do if the thing were true. It doesn’t have to be elaborate. Even a simple plan, like “Here’s how I would explain the failure to my team . . .” or “Here’s how I would begin my search for a new job . . .” goes a long way toward making you feel like you don’t need to rely on denial to cope with reality.

## Notice silver linings

Conceding an argument earns me credit. It makes me more credible in other cases, because I’ve demonstrated that I don’t stick to my guns just for the sake of it. It’s like I’m investing in my future ability to be convincing.

A silver lining to any mistake is the lesson you’re going to extract from the experience, which you can use to help save you from similar mistakes in the future.

Remember, the goal isn’t to convince yourself that your misfortune is actually a good thing. You’re not looking for a “sweet lemons” rationalization here. You’re recognizing a silver lining to the cloud, not trying to convince yourself the whole cloud is silver. But in many cases, that’s all you need—noticing the silver lining is enough to make you willing to accept the reality of the cloud.

## Focus on a different goal

A friend of mine named Jon cofounded a software company, and in the early days, spent a lot of time recruiting and interviewing potential new hires. He soon noticed something disturbing: When he came across a talented engineer who was interested in the position, he should have felt delighted. High-quality engineers can make all the difference to the success of a new software company. But instead, Jon felt something closer to disappointment or bitterness. He would scrutinize the engineer’s work, hoping to find an excuse to reject it.

Reflecting on his behavior, Jon realized: I’ve always prided myself on being the best programmer in the room. That’s why he was motivated to denigrate his “competition,” as a coping strategy to protect his self-esteem.

Jon knew that his goal of needing to be the best programmer in the room was unrealistic, not to mention very counterproductive for his fledgling company. So he decided to redirect his focus and revise his goal: Rather than priding himself on being a great programmer, he decided to start priding himself on being an astute judge of programming talent. That was a satisfying enough substitute for the original goal, and actually helpful for hiring instead of counterproductive.

## Things could be worse

In his history of the AIDS crisis, How to Survive a Plague, David France profiles a small group of activists called the Treatment Action Group. They had been following the drug-testing process closely and knew that the odds of finding a miracle drug right away were slim. When the bad news about AZT broke in the summer of 1993, they were disappointed—but not crushed.

Most of the activists in the Treatment Action Group were HIV-positive themselves. How had they kept up their spirits despite their realism about the chance of a cure? In part, by focusing on their gratitude for the things that could have been worse. France describes a meeting during that dispiriting summer at which one of the activists, a man named Peter Staley, said:

> Maybe that is our future, that we’re gonna watch each other die. And that’s going to be awful, if that’s the case. It’s already been awful, so there’s not too much we can do about that . . . I’m just—you know, I really honestly feel glad that I’ve got people to be with. Not many people have that

## Does research show that self-deceived people are happier?

Of course, the fact that the “self-deception causes happiness” research is fatally flawed doesn’t prove that self-deception can’t cause happiness. It clearly can, in many cases. It just comes with the downside of eroding your judgment. And given that there are so many ways to cope that don’t involve self-deception, why settle?

---

With practice, you develop your own tool kit of coping strategies that work for you. Just remember: don’t settle! Your ability to see clearly is precious, and you should be reluctant to sacrifice it in exchange for emotional comfort. The good news is that you don’t have to.

---

# Motivation Without Self-Deception

The self-belief model of success: If you convince yourself that you will succeed, you’ll be motivated to attempt hard things and persist in the face of setbacks, such that eventually your optimism will be self-fulfilling. Conversely, if you acknowledge the long odds facing you, or contemplate the possibility of failure, you’ll be too discouraged to try, and your pessimism will be a self-fulfilling prophecy as well.

> Have faith that you can successfully make it, and your feet are nerved to its accomplishment. But mistrust yourself, and think of all the sweet things you have heard the scientists say of maybes, and you will hesitate so long that, at last, all unstrung and trembling, and launching yourself in a moment of despair, you roll in the abyss. - William James

Many situations in our lives are like this, he argued. Choosing to have faith in your success, irrespective of the risk or difficulty, is the only way to summon the will to succeed. Is James right? If you could press a button and become irrationally optimistic about your chances of success—should you?

## An accurate picture of your odds helps you choose between goals

To weigh all the factors successfully, you need an accurate picture of what the odds actually are.

This is the biggest problem with the self-belief approach to motivation. Because you’re not supposed to think realistically about risk, it becomes impossible to ask yourself questions like, “Is this goal desirable enough to be worth the risk?” and “Are there any other goals that would be similarly desirable but require less risk?” It implicitly assumes that you don’t need to make any decisions; that you’ve already found the one right path, and there are no other options out there worth weighing.

Notice that in William James’s story of the perilous leap on the mountain—his argument for the value of irrational self-belief—he has constructed the example such that there’s zero decision-making involved. You aren’t given the opportunity to compare multiple options or brainstorm ideas you might have missed. The only thing you can do is try to execute the jump successfully.

In such a situation, where there is only one path available to you, maybe having a realistic picture of your odds of success on that path isn’t very useful. But how often does such a situation actually occur? Even in a real-life mountain-climbing scenario, there’s never literally only one choice. Instead of attempting to leap to a nearby peak, you could try climbing down the side of the mountain. Alternately, you could stay put and hope for rescue. Whether either of those options is a better bet than jumping depends on your estimate of their relative chances of success.

And even though the rhetoric around “following your dream” makes it sound like everyone has one and only one dream, most people have more than one thing they enjoy and are good at, or could at least become good at. You’re doing yourself a disservice if you throw yourself into the pursuit of a goal without asking: “Is this goal worth pursuing, compared to other things I could do instead?”

---

At this point, you might be thinking: “Sure, an accurate picture of the odds is important when you’re choosing a path. But once you’ve already made your choice, then you should switch into irrational optimism for the execution phase.”

Of course, it’s not quite as simple as “switching into irrational optimism.” You can’t just do a thoughtful, realistic calculus of risk and then erase it from your memory. But suppose you could—should you?

## An accurate picture of the odds helps you adapt your plan over time

The reality is that there’s no clear divide between the “decision-making” and “execution” stages of pursuing a goal. Over time, your situation will change, or you’ll learn new information, and you’ll need to revise your estimate of the odds.

## An accurate picture of the odds helps you decide how much to stake on success

Venture capitalist Ben Horowitz argues, in The Hard Thing About Hard Things, that there’s no point in thinking about your odds of success when building a company. “When you are building a company, you must believe there is an answer and you cannot pay attention to your odds of finding it. You just have to find it,” he writes. “It matters not whether your chances are nine in ten or one in a thousand; your task is the same.”

But even if your task is the same, that still leaves the question of how much you should be willing to gamble on your ability to succeed at that task. If your company has a 9 in 10 chance at success, then it might well be worth it to stake your life savings on it. If your chances are closer to 1 in 1,000, you probably want to leave that nest egg untouched.

Having an accurate picture of the odds doesn’t ever stop being valuable. Still, that leaves us with a psychological challenge: If you have an accurate picture of the odds, how do you keep from getting discouraged? How do you motivate yourself to give it your all, while knowing there’s a significant chance your “all” won’t be enough in the end?

## Bets worth taking

When Musk’s friends told him that he would probably fail (SpaceX), he replied: “Well, I agree. I think we probably will fail.” In fact, he estimated that there was only about a 10 percent chance that a SpaceX craft would ever make it into orbit.

Two years later, Musk decided to invest most of the remainder of his PayPal profits into an electric car company, Tesla. That, too, he gave a roughly 10 percent chance of success.

The low odds Musk assigned to his own projects’ success left many people scratching their heads. In an appearance on 60 Minutes in 2014, interviewer Scott Pelley tried to understand Musk’s logic:

Elon Musk: Well, I didn’t really think Tesla would be successful. I thought we would most likely fail . . .

Scott Pelley: But you say you didn’t expect the company to be successful? Then why try?

Elon Musk: If something’s important enough you should try. Even if the probable outcome is failure.

Musk’s low expectation of success confounds people because they assume the only reason to do something is if it’s likely to succeed. But scouts aren’t motivated by the thought, “This is going to succeed.” They’re motivated by the thought, “This is a bet worth taking.”

---

Most people are already on board with the idea of a “bet worth taking,” in at least some contexts. To give a simple example, suppose someone offered you a bet in which you roll a normal six-sided die. If it lands on a six, you win $200; if not, you lose $10. Should you take it?

Almost certainly. This is a good bet for you—and you can see exactly how good it is by calculating its expected value. That’s the average amount a bet pays out each time, if you were to take it an infinite number of times.

To calculate a bet’s expected value, multiply the probability of each outcome by its value and then add up those results. For this bet, the expected value would be:

`([¹⁄₆ probability of winning] × $200) + ([⁵⁄₆ probability of losing] × −$10) = $33.33 − $8.33 = $25 `
In other words, if you took this bet many times, the average amount you would win each time is about $25. Not bad money for simply rolling a die! This is a great bet to take, even though the most likely outcome is failure.

Evaluating the probabilities involved in a real-life bet, like starting a company, is a much messier, more subjective endeavor. The possible outcomes aren’t well defined the way they are in the case of the die roll. Their corresponding probabilities are subjective. And their “value” involves many factors besides money: How much enjoyment would you get out of running a company? Would it leave you with useful connections and skills, even if it failed? How much of your time would it take up? How much social cachet (or stigma) would it involve?

**MUSK’S THINKING ABOUT THE TESLA AND SPACEX BETS**

| **Probability**       | **Value**                                                                                                            |
| --------------------- | -------------------------------------------------------------------------------------------------------------------- |
| 10% chance of success | The company makes a big dent in one of the most pressing problems facing humanity (sustainability, space travel).    |
| 90% chance of failure | Musk loses his investment, but isn’t personally ruined. The company probably makes a bit of progress on the problem. |

Overall, both Tesla and SpaceX seemed like good bets to him—even though the most likely outcome for each was failure.

Another way to think about whether a bet is positive expected value is to imagine taking it many times. Would the value of the expected successes outweigh the value of the expected failures? Over the course of a lifetime, someone like Elon Musk probably has the time and money to attempt at least ten companies like Tesla and SpaceX. If his best guess is that nine of those ten companies will be failures, then the key question is: Would it be worth failing nine times in exchange for one big success?

In reality, you almost never get to repeat the exact same bet many times. But you’ll have the opportunity to make many different bets over the course of your life. You’ll face bets at your company and in your career more broadly; bets on investment opportunities; chances to bet on trusting another person, or making a difficult ask, or pushing your comfort zone. And the more positive expected value bets you make, the more confident you can be that you’ll end up ahead overall, even if each individual bet is far from a sure thing.

## Accepting variance gives you equanimity

Almost everyone, when asked to explain their success, gives a causal explanation like, “My extra practice is finally starting to pay off” or “It’s because I believed in myself.” How often do you hear someone chalk their own success up to “random variation”?

![The psychological effect of expecting variance](https://user-images.githubusercontent.com/31969517/120054198-2c681c00-c04c-11eb-9dce-15a9e936cda3.png)

The emotional toll of variance is actually worse than this graph suggests. We’re loss averse, meaning that the pain of a loss is greater than the pleasure of a similarly sized gain. Therefore, if you don’t build variance into your expectations, the low points on the spiky line graph will feel even lower than they are.

It might be motivating to believe with absolute certainty that you’re going to win, but it’s not realistic—there’s always some element of chance involved, in any endeavor. Over time, your outcomes will fluctuate; some of your bets will turn out well, and many will turn out poorly.

But as long as you continue making positive expected value bets, that variance will mostly wash out in the long run. Building that variance into your expectations has the nice side effect of giving you equanimity. Instead of being elated when your bets pay off, and crushed when they don’t, your emotions will be tied to the trend line underneath the variance.

> The goal isn’t to attribute everything to luck. It’s to do your best to mentally separate out the role that luck plays in your results from the role that your decision-making plays, and to judge yourself based on the latter.

Here’s an example of Bauer doing a postmortem of his pitching in one game:

Not a great pitch, but I defend the logic behind throwing it. Walked [Jason] Castro, not a good idea. Then, tried to get [Brian] Dozier on a fastball away, came back, good pitch, but he hit it.

Notice how he gives himself credit, then blame, then credit—all based on the quality of his pitching choices, independently of how they turned out.

## Coming to terms with risk

Bezos imagined being eighty years old and looking back at his life choices. Missing out on his 1994 Wall Street bonus wasn’t the kind of thing he would care about decades later. But passing up the chance to participate in the growth of the internet absolutely was. “If it failed, fine,” he decided. “I would be very proud of the fact when I’m 80 that I tried.” That’s what clinched his decision to take the plunge, quit his job, and start the company that would become Amazon.

The “self-belief” model of motivation assumes that if you acknowledge the possibility of failure, then you’ll be too demoralized or afraid to take risks. In that model, people who believe that failure is unthinkable are the ones who try the hardest to succeed. Yet in practice, things often seem to work the other way around—accepting the possibility of failure in advance is liberating. It makes you bold, not timid. It’s what gives you the courage to take the risks required to achieve something big.

When one interviewer praised Elon Musk for being fearless in starting companies that other people think are crazy, Musk admitted that he actually feels fear very strongly. He’s just learned to manage that fear by coming to terms with the probability of failure. “Something that can be helpful is fatalism, to some degree,” he explained. “If you just accept the probabilities, then that diminishes fear. So in starting SpaceX, I thought the odds of success were less than 10 percent, and I just accepted that probably I would lose everything.”

In moments when you're deciding what risks to take or stepping back to reflect on their life choices, being able to feel satisfied with the bet they’re taking—even if it fails—makes all the difference.

When I’m taking a bet I believe is worthwhile but risky I think about a line from a blog post I read that stayed with me: “You want to get into a mental state where if the bad outcome comes to pass, you will only nod your head and say ‘I knew this card was in the deck, and I knew the odds, and I would make the same bets again, given the same opportunities.’”

---

We have a choice of coping strategies for dealing with emotions like anxiety, disappointment, regret, and fear. Some coping strategies involve self-deception, and some don’t—so why settle for the former?

The same logic applies to our strategies for motivating ourselves to be ambitious, take risks, and persevere when things get tough. The soldier approach to motivation requires you to believe things that aren’t true—that your odds of success don’t matter as long as you believe in yourself, that failure is not an option, that “luck” is irrelevant.

Soldier morale can be effective, at least in the short term. But it’s a brittle kind of morale, one that requires you to avoid or rationalize away new information that could threaten your ability to keep believing in success.

Scouts rely on a different kind of morale. Instead of being motivated by the promise of guaranteed success, a scout is motivated by the knowledge that they’re making a smart bet, which they can feel good about having made whether or not it succeeds. Even if a particular bet has a low probability of success, they know that their overall probability of success in the long run is much higher, as long as they keep making good bets. They’re motivated by the knowledge that downturns are inevitable, but will wash out in the long run; that although failure is possible, it’s also tolerable.

The scout approach to morale doesn’t ask you to sacrifice your ability to make clear-eyed decisions. And it’s a robust kind of morale, one that doesn’t require protection from reality, because it’s rooted in truth.

# Influence without overconfidence

The common wisdom is that the more confidence you can muster in your beliefs, the more influential you will be. Confidence is magnetic. It invites people to listen to you, follow you, and trust that you know what you’re doing. If you look up advice on how to be influential or persuasive, you’ll find lots of exhortations to **believe in yourself**

This would seem to bode poorly for scouts; if you’re being intellectually honest, you’re not going to have certainty about everything. Fortunately, the common wisdom isn’t quite right.

## Two types of confidence

One is epistemic confidence, or certainty—how sure you are about what’s true. If you say, “I’m 99 percent sure he is lying,” or “I guarantee this will work,” or “There’s no way the Republicans can win,” you’re displaying a lot of epistemic confidence.

Separately, there’s social confidence, or self-assurance: Are you at ease in social situations? Do you act like you deserve to be there, like you’re secure in yourself and your role in the group? Do you speak as if you’re worth listening to?

![Two types of confidence](https://user-images.githubusercontent.com/31969517/120054214-3853de00-c04c-11eb-931c-636ba27e8b02.png)

We tend to conflate epistemic confidence and social confidence, treating them as if they’re a package deal. It’s easy to picture someone with both types of confidence, such as a leader pumping up his team with an inspiring pep talk about how there’s no doubt in his mind that they’re going to succeed. It’s also easy to picture someone lacking in both types of confidence, stammering nervously, “Uh, I’m really not sure what we should do here . . .”

But epistemic confidence and social confidence don’t have to be a package deal. Just look at Benjamin Franklin. He was brimming with social confidence—famously charming, witty, and ebullient, he made friends and launched new institutions his entire life. He was basically a celebrity in France, where he was constantly surrounded by adoring women who called him “Cher Papa” (“Dear Papa”).

Yet Franklin paired his abundance of social confidence with an intentional lack of epistemic confidence. It was a practice he had started when he was young, after noticing that people were more likely to reject his arguments when he used firm language like certainly and undoubtedly. So Franklin trained himself to avoid those expressions, prefacing his statements instead with caveats like “I think . . .” or “If I’m not mistaken . . .” or “It appears to me at present . . .”

It was a tough habit to stick to at first. One of Franklin’s favorite pastimes as a young man had been proving other people wrong, or what might nowadays be called “destroying” people in arguments. But the habit soon got easier as he started noticing how much more receptive people were to his opinions when he expressed them gently.

## People judge you on social confidence, not epistemic confidence

When it comes to the impression you make on other people, being self-assured is more important than expressing certainty.

People sometimes bemoan the fact that “superficial” things like posture and voice make such a difference in how we judge each other. But on the bright side, that means that projecting competence doesn’t require self-deception. You can boost your social confidence through practice speaking up in groups, hiring a speech coach, dressing better, improving your posture—all without compromising your ability to see things clearly.

The founding of Amazon is a case in point for the precedence of social confidence over epistemic confidence. The company’s big break came in the spring of 1996, when it received a visit from John Doerr, a partner at Kleiner Perkins Caufield & Byers, one of the most prestigious venture capital firms in Silicon Valley (now just Kleiner Perkins). Doerr left that meeting wowed by Amazon and ready to invest. Even better, the interest from a high-profile venture capitalist triggered a bidding war that drove Amazon’s valuation up from $10 million to $60 million.

So, what exactly sold Doerr on Amazon? I’ll let him explain: “I walked into the door and this guy with a boisterous laugh who was just exuding energy comes bounding down the steps. In that moment, I wanted to be in business with Jeff.”

## Two kinds of uncertainty

If a doctor says, “I’m not sure what’s causing this,” it’s reasonable to wonder whether a better, more experienced doctor would be able to diagnose you.

However, if the doctors sound like experts even while they’re giving an uncertain diagnosis. They’re offering useful context and they’re giving informative estimates rather than simply saying they don’t know.

![Two kinds of uncertainty](https://user-images.githubusercontent.com/31969517/120054222-473a9080-c04c-11eb-8786-35e20a432ee3.png)

When people claim that “admitting uncertainty” makes you look bad, they’re invariably conflating these two very different kinds of uncertainty: uncertainty “in you,” caused by your own ignorance or lack of experience, and uncertainty “in the world,” caused by the fact that reality is messy and unpredictable. The former is often taken as a bad sign about someone’s expertise, and justifiably so. But the latter is not—especially if you follow three rules for communicating uncertainty:

- ## 1. Show that uncertainty is justified

  Sometimes your audience won’t be aware of how much uncertainty exists “in the world” on the topic you’re speaking about, and they’ll expect you to give answers with more certainty than is actually possible. That’s okay; you just need to set their expectations.

  In fact, if you show that certainty is unrealistic, you can be more persuasive than someone who states everything with 100 percent certainty. When an attorney meets with a potential client for the first time, the client always asks how much money they can expect to be awarded. It’s tempting for the attorney to give a confident, optimistic estimate, but the reality is that he doesn’t yet have enough information to go on. Instead, here’s what a prosecutor interviewed in How Leading Lawyers Think says in such a situation: “I tell them, ‘Any attorney who answers that either is lying to you or does not know what he’s doing, and you should run like hell.’”

- ## 2. Give informed estimates

  Even if reality is messy and it’s impossible to know the right answer with confidence, you can at least be confident in your analysis.

Showing that you’re well-informed and well prepared on a given topic doesn’t require you to overstate how much certainty is possible on that topic.

- ## 3. Have a plan

  One reason people don’t like hearing uncertain answers is that it leaves them at a loss for how to act. You can reassure them by following up your uncertainty with a plan or recommendation.

Having a plan means being able to make a strong case for what you are going to do to make your business a good bet—a bet that you feel confident about taking, and that other people can feel confident investing in, even though success isn’t guaranteed. In his 1999 CNBC interview, after acknowledging that Amazon was a risk, Jeff Bezos went on to explain why it was nevertheless a good risk to take:

It’s very, very hard to predict. But I believe that if you can focus obsessively enough on customer experience, selection, ease of use, low prices, more information to make purchase decisions with, if you can give customers all that plus great customer service . . . I think you have a good chance. And that’s what we’re trying to do.

## You don't need to promise success to be inspiring

“You don’t have to psych them up by lying or by being overconfident about the chance of success.”

You can set ambitious goals. You can paint a vivid picture of the world you want to create. You can speak from the heart about why you personally care about this issue. All of those things can be inspiring, and none of them require you to make unrealistic claims.

---

you don’t need to hold your opinions with 100 percent certainty in order to seem confident and competent. People simply aren’t paying that much attention to how much epistemic confidence you express. They’re paying attention to how you act, to your body language, tone, and other aspects of your social confidence, all of which are things you can cultivate without sacrificing your calibration.

Second, expressing uncertainty isn’t necessarily a bad thing. It depends on whether the uncertainty is “in you” or “in the world.” If you can demonstrate a strong command of the topic and speak with ease about your analysis and your plan, you’ll seem like more of an expert, not less.

Third, you can be inspiring without overpromising. You can paint a picture of the world you’re trying to create, or why your mission is important, or how your product has helped people, without claiming you’re guaranteed to succeed. There are lots of ways to get people excited that don’t require you to lie to others or to yourself.

whatever your goal, there’s probably a way to get it that doesn’t require you to believe false things. From now on, whenever you hear someone claim that you need to self-deceive in order to be happy, motivated, or influential, you should raise a skeptical eyebrow. There are multiple paths to any goal, some of which involve self-deception and some of which don’t. It may take a little more care and practice to find the latter, but in the long run, it’s well worth it.

There are lots of ways to change the game board you’re playing on so that you end up with better choices, instead of simply resigning yourself to picking the least-bad choice currently in front of you.

# How to Be Wrong

## Change your mind a little at a time

You’re probably already comfortable with the idea of changing your mind incrementally in some contexts. When you submit a job application, you might figure you have about a 5 percent chance of ultimately getting an offer. After they call you to set up an in-person job interview, your estimate might rise to about 10 percent. During the interview, if you feel like you’re hitting it out of the park, maybe your confidence in getting an offer rises to 30 percent. If you haven’t heard from them for a couple of weeks after your interview, your confidence might fall back down to 20 percent.

What’s much rarer is for someone to do the same thing with their opinions about politics, morality, or other charged topics.

Changing your mind frequently, especially about important beliefs, might sound mentally and emotionally taxing. But, in a way, it’s less stressful than the alternative. If you see the world in binary black-and-white terms, then what happens when you encounter evidence against one of your beliefs? The stakes are high: you have to find a way to dismiss the evidence, because if you can’t, your entire belief is in jeopardy.

If instead you see the world in shades of gray, and you think of “changing your mind” as an incremental shift, then the experience of encountering evidence against one of your beliefs is very different.

## Recognizing you were wrong makes you better at being right

This is another reason superforecasters are much happier to think about what they got wrong—they know that analyzing their errors is an opportunity to hone their technique. Lessons like “Don’t assume world leaders would react the same way as you” are like power-ups, upgrades to your mental arsenal that make you smarter going forward.

## Learning domain-general lessons

When a forecaster recognizes he was wrong, it helps him make better forecasts. When an investor recognizes he was wrong, it helps him make better investments.

the biggest benefits of noticing your errors: the opportunity to improve your judgment in general.

Domain-general, meaning that they apply to a wide variety of different domains, as opposed to domain-specific lessons that apply only to a single domain. Domain-general lessons are about how the world works, or how your own brain works, and about the kinds of biases that tend to influence your judgment.

> It’s easy to be fooled by cherry-picked evidence.

> If it seems like someone is saying something dumb, I might be misunderstanding them.

> Even when I feel certain, there’s still a chance I’m wrong.

ou might think these principles sound obvious and that you know them already. But “knowing” a principle, in the sense that you read it and say, “Yes, I know that,” is different from having internalized it in a way that actually changes how you think.

such knowledge doesn’t really become part of you until you’ve derived it for yourself by going through the experience of realizing you were wrong, asking yourself why, and seeing the effect of the bias at work.

Even when you’re wrong about something random or trivial, there are still generally useful lessons to be had.

scouts think about error differently from most people. First, they revise their opinions incrementally over time, which makes it easier to be open to evidence against their beliefs. Second, they view errors as opportunities to hone their skill at getting things right, which makes the experience of realizing “I was wrong” feel valuable, rather than just painful.

## Admitting a mistake vs Updating

An Assumption about changing your mind is that its humbling. That saying “I was wrong” is equivalent to saying “I screwed up”—something you confess with contrition or sheepishness. Indeed, that’s the standard way of thinking about being wrong. Even my fellow cheerleaders for changing one’s mind tend to say things like, “It’s okay to admit you were wrong!” While I appreciate the intentions behind this advice, I’m not sure it makes things much better. The word admit makes it sound like you screwed up but that you deserve to be forgiven because you’re only human. It doesn’t question the premise that being wrong means you screwed up.

couts reject that premise. You’ve learned new information and come to a new conclusion, but that doesn’t mean you were wrong to believe differently in the past. The only reason to be contrite is if you were negligent in some way. Did you get something wrong because you followed a process you should have known was bad? Were you willfully blind or stubborn or careless?

But most of the time, being wrong doesn’t mean you did something wrong. It’s not something you need to apologize for, and the appropriate attitude to have about it is neither defensive nor humbly self-flagellating, but matter-of-fact.

Instead of “admitting a mistake,” scouts will sometimes talk about “updating.” That’s a reference to Bayesian updating, a technical term from probability theory for the correct way to revise a probability after learning new information. The way people use the word updating colloquially isn’t nearly so precise, but it still gestures at the spirit of revising one’s beliefs in response to new evidence and arguments.

> Software engineer and product manager Devon Zuegel encourages readers to view her blog posts not as her permanent opinions, but instead as “a stream of thoughts, caught in the middle of updates."

If you start to think in terms of “updating” instead of “admitting you were wrong,” you may find that it takes a lot of friction out of the process. An update is routine. Low-key. It’s the opposite of an overwrought confession of sin. An update makes something better or more current without implying that its previous form was a failure.

> “As I’ve gotten older, it’s gotten easier to be wrong, “Not even to be wrong. It’s just an update: I learned this new thing . . . what’s the issue?” - Emmett Shear

## If you're not changing your mind, you're doing something wrong

One thing that makes the Humane League unusual is their commitment to the premise that they’re always at least a little bit wrong.

Switching from one type of strategy or cause to another. In its early years, the Humane League concentrated on flashy demonstrations like picketing the homes of scientists involved in animal testing. But they found that this strategy was too alienating to be effective, and the number of animals it could save even in a best-case scenario wasn’t very high. That’s why they ultimately shifted their focus from lab animals to farm animals, and persuaded Unilever, which supplies 95 percent of the United States’ eggs, to agree to stop killing male chicks.

Knowing that you’re fallible doesn’t magically prevent you from being wrong. But it does allow you to set expectations early and often, which can make it easier to accept when you are wrong. Coman-Hidy says, “My intuition is that if you bring up these biases a lot—that we are always going to think that we’re right, that we’re always thinking what we’re doing is the best and most important thing to be doing . . . it makes it an easier pill to swallow when inevitably something better comes along. Because you’ve kind of inoculated yourself against the ‘horror’ of having been suboptimal for some period of time.”

Discovering you were wrong is an update, not a failure, and your worldview is a living document meant to be revised. In the next chapter we’ll explore another key facet of changing your mind. Now that you’ve gotten good at being wrong, it’s time to get good at being confused.

# Escape your echo chamber

## How not to learn from disagreement

Learning from disagreement isn't hopeless—it’s just we’re going about it all wrong.

Our mistake lies in how we select the sources we listen to. By default, we end up listening to people who initiate disagreements with us, as well as the public figures and media outlets who are the most popular representatives of the other side. Those are not very promising selection criteria.Second, what kind of people or media are likely to become popular representatives of an ideology? The ones who do things like cheering for their side and mocking or caricaturing the other side—i.e., you.

To give yourself the best chance of learning from disagreement, you should be listening to people who make it _easier_ to be open to their arguments, not harder. People you like or respect, even if you don’t agree with them. People with whom you have some common ground—intellectual premises, or a core value that you share—even though you disagree with them on other issues. People whom you consider reasonable, who acknowledge nuance and areas of uncertainty, and who argue in good faith.

## Listen to people you find reasonable

Rashid had considered himself an “antifeminist” when he first joined  r/FeMRADebates, but he has since dropped that label. What changed his mind? Talking with feminists who were arguing in good faith.

On the other side of the aisle, one of the group’s feminist founders began to see some flaws in concepts from feminist theory, such as “patriarchy.” She also came to care much more about some problems MRAs emphasize.

## Liste to people you share intellecual common ground with

Knowing that you have intellectual common ground with someone makes you more receptive to their arguments right off the bat. It also makes it possible for them to explain their side in your “language.”

## Listen to people who share your goals

Kelsey is  an atheist. And one of her good friends Jen is a practicing Catholic. That’s a large difference in beliefs, one that often makes disagreements intractable, When one person’s moral positions stem from religious premises that another person doesn’t share, it’s hard to know how to make progress.

But one thing Kelsey and Jen do share is a desire to make the world a better place as effectively as possible. They’re both part of the effective altruism movement, which is devoted to finding high-impact, evidence-based ways of doing good. That shared goal creates a sense of camaraderie and trust between them, making Kelsey more willing to listen open-mindedly to Jen’s perspective than she might otherwise have been.

At first, Kelsey had been unconflictedly pro-choice. Now, after many conversations with Jen, Kelsey is somewhat more sympathetic to the pro-life position. Kelsey is still strongly in favor of legal abortion. But she now takes more seriously the possibility that abortions are a bad outcome, one that we should be trying harder to prevent.

That shift wouldn’t have happened if Kelsey hadn’t made a genuine effort to understand Jen’s perspective—and that wouldn’t have happened if Kelsey hadn’t felt like Jen was her ally in a fight to make the world better, someone who cares about many of the same things as Kelsey does. Feeling like you’re on the same team in some important way can make it possible to learn from each other, even when your worldviews are otherwise very different.

## The problem with a "Team of rivals"

Dissent isn’t all that useful from people you don’t respect or from people who don’t even share enough common ground with you to agree that you’re supposed to be on the same team.

## It's harder than you think

We need to lower our expectations, by a lot. Even under ideal conditions in which everyone is well-informed, reasonable, and making a good-faith effort to explain their views and understand the other side, learning from disagreements is still hard (and conditions are almost never ideal). Here are three reasons why:

### 1. We misunderstand each other’s views

While on a trip to Cairo, blogger Scott Alexander got into a pleasant conversation with a Muslim girl in a café. When she mentioned something about crazy people who believe in evolution, Alexander admitted that he was one of those “crazy people.”

The girl was shocked. She replied, “But . . . monkeys don’t change into humans. What on Earth makes you think monkeys can change into humans?”Alexander tried to explain that the change from ape to human was a very gradual one, which occurred over many generations, and he recommended some books that might do a better job explaining the process than he could. But it was clear that she was still not buying it.

If you’re already familiar with the theory of evolution, it’s obvious to you that the girl in the café was misunderstanding it. But are you sure that none of the absurd-sounding ideas you’ve dismissed in the past aren’t also misunderstandings of the real thing? Even correct ideas often sound wrong when you first hear them. The thirty-second version of an explanation is inevitably simplistic, leaving out important clarifications and nuance. There’s background context you’re missing, words being used in different ways than you’re used to, and more.

### 2. Bad arguments inoculate us against good arguments

When we do encounter a good argument that’s new to us, we often mistake it for a bad argument we’re already familiar with.

Gary Klein’s work has been very helpful to me in understanding how real-world decision-making works, and in recognizing some of the shortcomings in academic studies of decision-making.

Yet I ignored Klein’s work for years after I first heard of him. That’s because he talks about the “power of intuition,” which made me associate him with the kind of people who exalt intuition as a pseudo-mystical sixth sense that deserves precedence over all other forms of evidence, including science. That’s not Klein’s view. By “intuition” he’s simply referring to our brains’ built-in pattern-matching abilities. But because I had encountered so many people saying things like “I don’t care what science says—my intuition tells me ghosts are real,” I automatically lumped Klein in with them.

### 3. Our beliefs are interdependent—changing one requires changing others

Suppose that Alice believes that climate change is a serious problem, and she’s talking to Kevin, who disagrees. Alice could show Kevin an article saying that climate science models have made accurate predictions, but that’s not likely to change Kevin’s mind—even if Kevin is in scout mindset.

That’s because our beliefs are all interconnected, like a web. The belief “climate change isn’t real” is supported by other beliefs Kevin holds about how the world works and which sources are trustworthy. For Kevin to significantly update the belief “climate change isn’t real,” he’ll have to also update a few of his associated beliefs, such as “Climate change skeptic media outlets are more trustworthy than the mainstream media,” or “Smart people don’t buy the climate science consensus.” That can happen, but it will take a lot more evidence than a single article from a news source Kevin doesn’t currently trust.

---

People expect it to be easy to understand disagreements and are unpleasantly surprised when they fail. But the reality is that it’s hard even under the best conditions, and you should be pleasantly surprised when you actually succeed. Listening to views you disagree with, and taking them seriously enough to have a shot at changing your mind, requires mental effort, emotional effort, and, above all, patience. You have to be willing to say to yourself, “It _seems_ like this person is wrong, but maybe I’m misunderstanding him—let me check,” or “I still don’t agree, but maybe over time I’ll start to see examples of what she’s talking about.”

Why make a difficult task even more difficult by hearing those views from people who are unreasonable, who mock your views, and with whom you don’t share any common ground? Why not give yourself the best possible shot of changing your mind, or at least being able to appreciate how a reasonable person could disagree with you? As Kelsey (the atheist journalist with the Catholic friend) put it: “If reading someone does not make me feel more compassion towards their perspective, then I keep looking.”